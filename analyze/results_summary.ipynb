{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results Summary\n",
    "\n",
    "This notebook consolidates all benchmark results into three tables:\n",
    "1. **Debias Methods** - Selection bias correction methods\n",
    "2. **PU Methods** - Positive-Unlabeled learning methods\n",
    "3. **Debias+PU Methods** - Combined methods (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model categories\n",
    "DEBIAS_MODELS = [\n",
    "    'naive', 'ips', 'dr', 'mtips', 'mtdr', 'sdr', 'sdr2',\n",
    "    'ome_ips', 'ome_dr', 'co_teaching', 'cvib', 'codis',\n",
    "    'kmeidtm', 'labelwave', 'eps_softmax', 'robust_dividemix'\n",
    "]\n",
    "\n",
    "PU_MODELS = [\n",
    "    'bpr', 'ubpr', 'cubpr', 'nnpu', 'upu', 'pu_naive',\n",
    "    'uprl', 'wmf', 'rmf', 'ncrmf'\n",
    "]\n",
    "\n",
    "# Placeholder for combined methods\n",
    "DEBIAS_PU_MODELS = []\n",
    "\n",
    "# Datasets and metrics\n",
    "DATASETS = ['hs', 'saferlhf', 'ufb']\n",
    "METRICS = ['AUROC', 'NLL', 'NDCG', 'Recall']\n",
    "\n",
    "# Results directory\n",
    "RESULTS_DIR = Path('../results/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(cache_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Load all performance.yaml files from the cache directory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {(model_name, dataset_name): {metric: value, ...}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for perf_file in cache_dir.glob('*/*/performance.yaml'):\n",
    "        model = perf_file.parent.parent.name\n",
    "        dataset = perf_file.parent.name\n",
    "        \n",
    "        # Skip debug directories\n",
    "        if 'debug' in dataset:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(perf_file) as f:\n",
    "                data = yaml.safe_load(f)\n",
    "            results[(model, dataset)] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load {perf_file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def build_results_table(results: dict, models: list, datasets: list, metrics: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a results table for a specific set of models.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of results from load_all_results\n",
    "        models: List of model names to include\n",
    "        datasets: List of dataset names\n",
    "        metrics: List of metric names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with models as rows and dataset_metric as columns\n",
    "    \"\"\"\n",
    "    # Create column names\n",
    "    columns = []\n",
    "    for dataset in datasets:\n",
    "        for metric in metrics:\n",
    "            columns.append(f\"{dataset}_{metric}\")\n",
    "    \n",
    "    # Build data\n",
    "    data = []\n",
    "    for model in models:\n",
    "        row = {'Model': model}\n",
    "        for dataset in datasets:\n",
    "            for metric in metrics:\n",
    "                col_name = f\"{dataset}_{metric}\"\n",
    "                key = (model, dataset)\n",
    "                if key in results:\n",
    "                    # Metrics in yaml are like \"AUROC on test\"\n",
    "                    metric_key = f\"{metric} on test\"\n",
    "                    value = results[key].get(metric_key, None)\n",
    "                    row[col_name] = round(value, 4) if value is not None else None\n",
    "                else:\n",
    "                    row[col_name] = None\n",
    "        data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index('Model')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load all results\n",
    "all_results = load_all_results(RESULTS_DIR)\n",
    "print(f\"Loaded {len(all_results)} result files\")\n",
    "print(f\"Models found: {set(k[0] for k in all_results.keys())}\")\n",
    "print(f\"Datasets found: {set(k[1] for k in all_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1: Debias Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debias_table = build_results_table(all_results, DEBIAS_MODELS, DATASETS, METRICS)\n",
    "display(debias_table.style.format(precision=4, na_rep='-').highlight_max(axis=0, props='font-weight: bold'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2: PU Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu_table = build_results_table(all_results, PU_MODELS, DATASETS, METRICS)\n",
    "display(pu_table.style.format(precision=4, na_rep='-').highlight_max(axis=0, props='font-weight: bold'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: Debias+PU Methods (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBIAS_PU_MODELS:\n",
    "    debias_pu_table = build_results_table(all_results, DEBIAS_PU_MODELS, DATASETS, METRICS)\n",
    "    display(debias_pu_table.style.format(precision=4, na_rep='-').highlight_max(axis=0, props='font-weight: bold'))\n",
    "else:\n",
    "    print(\"No Debias+PU models defined yet. Add model names to DEBIAS_PU_MODELS list when available.\")\n",
    "    # Create empty placeholder table\n",
    "    columns = [f\"{d}_{m}\" for d in DATASETS for m in METRICS]\n",
    "    debias_pu_table = pd.DataFrame(columns=columns)\n",
    "    debias_pu_table.index.name = 'Model'\n",
    "    display(debias_pu_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to export tables to CSV\n",
    "# debias_table.to_csv('debias_results.csv')\n",
    "# pu_table.to_csv('pu_results.csv')\n",
    "# debias_pu_table.to_csv('debias_pu_results.csv')\n",
    "# print(\"Tables exported to CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
