{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results Summary\n",
    "\n",
    "This notebook consolidates all benchmark results into three tables:\n",
    "1. **Debias Methods** - Selection bias correction methods\n",
    "2. **PU Methods** - Positive-Unlabeled learning methods\n",
    "3. **Debias+PU Methods** - Combined methods (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model categories\nDEBIAS_MODELS = [\n    'naive', 'ips', 'dr', 'mtips', 'mtdr', 'sdr', 'sdr2',\n    'ome_ips', 'ome_dr', 'co_teaching', 'cvib', 'codis',\n    'kmeidtm', 'labelwave', 'eps_softmax', 'robust_dividemix'\n]\n\nPU_MODELS = [\n    'bpr', 'ubpr', 'cubpr', 'nnpu', 'upu', 'pu_naive',\n    'uprl', 'wmf', 'rmf', 'ncrmf'\n]\n\n# Placeholder for combined methods\nDEBIAS_PU_MODELS = []\n\n# Datasets and metrics\nDATASETS = ['hs', 'saferlhf', 'ufb']\nMETRICS = ['AUROC', 'NLL', 'MAE', 'RMSE']\n\n# Results directory\nRESULTS_DIR = Path('../results/cache')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_all_results(cache_dir: Path) -> dict:\n    \"\"\"\n    Load all performance.yaml files from the cache directory.\n    \n    Returns:\n        dict: {(model_name, dataset_name): {metric: value, ...}}\n    \"\"\"\n    results = {}\n    \n    for perf_file in cache_dir.glob('*/*/performance.yaml'):\n        model = perf_file.parent.parent.name\n        dataset = perf_file.parent.name\n        \n        # Skip debug directories\n        if 'debug' in dataset:\n            continue\n        \n        try:\n            with open(perf_file) as f:\n                data = yaml.safe_load(f)\n            results[(model, dataset)] = data\n        except Exception as e:\n            print(f\"Warning: Failed to load {perf_file}: {e}\")\n    \n    return results\n\n\ndef build_results_table(results: dict, models: list, datasets: list, metrics: list) -> pd.DataFrame:\n    \"\"\"\n    Build a results table for a specific set of models with MultiIndex columns.\n    \n    Args:\n        results: Dictionary of results from load_all_results\n        models: List of model names to include\n        datasets: List of dataset names\n        metrics: List of metric names\n    \n    Returns:\n        DataFrame with models as rows and (dataset, metric) MultiIndex columns\n    \"\"\"\n    # Create MultiIndex columns\n    columns = pd.MultiIndex.from_product([datasets, metrics], names=['Dataset', 'Metric'])\n    \n    # Build data\n    data = []\n    for model in models:\n        row = []\n        for dataset in datasets:\n            for metric in metrics:\n                key = (model, dataset)\n                if key in results:\n                    metric_key = f\"{metric} on test\"\n                    value = results[key].get(metric_key, None)\n                    row.append(round(value, 4) if value is not None else None)\n                else:\n                    row.append(None)\n        data.append(row)\n    \n    df = pd.DataFrame(data, index=models, columns=columns)\n    df.index.name = 'Model'\n    \n    return df\n\n\ndef highlight_best(df: pd.DataFrame, lower_is_better: list = ['NLL', 'MAE', 'RMSE']):\n    \"\"\"\n    Highlight the best value in each column.\n    For metrics in lower_is_better, highlight the minimum; otherwise highlight the maximum.\n    \"\"\"\n    def highlight_col(s):\n        metric = s.name[1] if isinstance(s.name, tuple) else s.name\n        if metric in lower_is_better:\n            is_best = s == s.min()\n        else:\n            is_best = s == s.max()\n        return ['font-weight: bold' if v else '' for v in is_best]\n    \n    # Apply styles with centered Dataset header\n    styled = df.style.format(precision=4, na_rep='-').apply(highlight_col, axis=0)\n    # Center the top-level (Dataset) header\n    styled = styled.set_table_styles([\n        {'selector': 'th.col_heading.level0', 'props': [('text-align', 'center')]},\n    ])\n    return styled\n\n\n# Load all results\nall_results = load_all_results(RESULTS_DIR)\nprint(f\"Loaded {len(all_results)} result files\")\nprint(f\"Models found: {set(k[0] for k in all_results.keys())}\")\nprint(f\"Datasets found: {set(k[1] for k in all_results.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1: Debias Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "debias_table = build_results_table(all_results, DEBIAS_MODELS, DATASETS, METRICS)\ndisplay(highlight_best(debias_table))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2: PU Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pu_table = build_results_table(all_results, PU_MODELS, DATASETS, METRICS)\ndisplay(highlight_best(pu_table))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: Debias+PU Methods (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if DEBIAS_PU_MODELS:\n    debias_pu_table = build_results_table(all_results, DEBIAS_PU_MODELS, DATASETS, METRICS)\n    display(highlight_best(debias_pu_table))\nelse:\n    print(\"No Debias+PU models defined yet. Add model names to DEBIAS_PU_MODELS list when available.\")\n    # Create empty placeholder table with MultiIndex columns\n    columns = pd.MultiIndex.from_product([DATASETS, METRICS], names=['Dataset', 'Metric'])\n    debias_pu_table = pd.DataFrame(columns=columns)\n    debias_pu_table.index.name = 'Model'\n    display(debias_pu_table)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalrm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}